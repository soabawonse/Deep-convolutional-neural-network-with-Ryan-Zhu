{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Classification CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "from sklearn import decomposition\n",
    "from sklearn import manifold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from tqdm.notebook import tqdm, trange\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import copy\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Calculated mean: [tensor(0.4914), tensor(0.4822), tensor(0.4465)]\n",
      "Calculated std: [tensor(0.2470), tensor(0.2435), tensor(0.2616)]\n"
     ]
    }
   ],
   "source": [
    "ROOT = '../../data'\n",
    "\n",
    "train_data = datasets.CIFAR10(root=ROOT,\n",
    "                            train=True,\n",
    "                            download=True)\n",
    "\n",
    "train_data.data = torch.tensor(train_data.data)\n",
    "\n",
    "channels = train_data.data.split(1, dim=-1)\n",
    "channel_tensors = [channel.squeeze(-1) for channel in channels]\n",
    "\n",
    "means = [z.float().mean() / 255 for z in channel_tensors]\n",
    "stds = [z.float().std() / 255 for z in channel_tensors]\n",
    "\n",
    "\n",
    "print(f'Calculated mean: {means}')\n",
    "print(f'Calculated std: {stds}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = transforms.Compose([\n",
    "                            # transforms.RandomRotation(5, fill=(0,)),\n",
    "                            # transforms.RandomCrop(28, padding=2),\n",
    "                            transforms.ToTensor(),\n",
    "                            transforms.Normalize(mean=means, std=stds)\n",
    "                                      ])\n",
    "\n",
    "test_transforms = transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize(mean=means, std=stds)\n",
    "                                     ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_data = datasets.CIFAR10(root=ROOT,\n",
    "                            train=True,\n",
    "                            download=True,\n",
    "                            transform=train_transforms)\n",
    "\n",
    "test_data = datasets.CIFAR10(root=ROOT,\n",
    "                           train=False,\n",
    "                           download=True,\n",
    "                           transform=test_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "VALID_RATIO = 0.9\n",
    "\n",
    "n_train_examples = int(len(train_data) * VALID_RATIO)\n",
    "n_valid_examples = len(train_data) - n_train_examples\n",
    "\n",
    "train_data, valid_data = data.random_split(train_data,\n",
    "                                           [n_train_examples, n_valid_examples])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_data = copy.deepcopy(valid_data)\n",
    "valid_data.dataset.transform = test_transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 45000\n",
      "Number of validation examples: 5000\n",
      "Number of testing examples: 10000\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of training examples: {len(train_data)}')\n",
    "print(f'Number of validation examples: {len(valid_data)}')\n",
    "print(f'Number of testing examples: {len(test_data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "train_iterator = data.DataLoader(train_data, batch_size=BATCH_SIZE,shuffle = True)\n",
    "valid_iterator = data.DataLoader(valid_data, batch_size=BATCH_SIZE, shuffle = True)\n",
    "test_iterator = data.DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 4,941,066 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, output_classes=10):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.LazyConv2d(out_channels=128,kernel_size=11,padding=5)\n",
    "        self.conv2 = nn.LazyConv2d(out_channels = 256, kernel_size=7, padding=3)\n",
    "        self.conv3 = nn.LazyConv2d(out_channels = 512, kernel_size=3, padding = 1)\n",
    "        \n",
    "        self.MLP = nn.LazyLinear(out_features=1024)\n",
    "        self.end = nn.LazyLinear(out_features=output_classes)\n",
    "\n",
    "\n",
    "    def init_weights(self):\n",
    "        nn.init.xavier_uniform_(self.conv1.weight)\n",
    "        nn.init.xavier_uniform_(self.conv2.weight)\n",
    "        nn.init.xavier_uniform_(self.conv3.weight)\n",
    "        nn.init.xavier_uniform_(self.MLP.weight)\n",
    "        nn.init.xavier_uniform_(self.end.weight)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = nn.ReLU()(self.conv1(X))\n",
    "        X = nn.MaxPool2d(kernel_size=3,padding=1)(X)\n",
    "        X = nn.ReLU()(self.conv2(X))\n",
    "        X = nn.MaxPool2d(kernel_size=3, padding=1)(X)\n",
    "        X = nn.ReLU()(self.conv3(X))\n",
    "        X = nn.MaxPool2d(kernel_size=3, padding=1)(X)\n",
    "        X = nn.Flatten()(X)\n",
    "        X = nn.ReLU()(self.MLP(X))\n",
    "        X = self.end(X)\n",
    "        return X\n",
    "\n",
    "\n",
    "model = Model()\n",
    "model(torch.Tensor(1,3,32,32))\n",
    "\n",
    "def init_cnn(module: nn.Module):\n",
    "    if type(module) == nn.Linear or type(module) == nn.Conv2d:\n",
    "        nn.init.xavier_uniform_(module.weight)\n",
    "        \n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(model.parameters())\n",
    "loss = nn.CrossEntropyLoss()\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "loss = loss.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc(y_hat: torch.Tensor, y: torch.Tensor):\n",
    "    preds = y_hat.argmax(1, keepdim=True)\n",
    "    hits = preds.eq(y.view_as(preds)).sum()\n",
    "    return hits.float() / y.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, it, optim, loss, device):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.train() # set training mode\n",
    "\n",
    "    for (data, labels) in tqdm(it, desc=\"Training\", leave=False):\n",
    "        data = data.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optim.zero_grad()\n",
    "        y_hat = model(data)\n",
    "        batch_loss = loss(y_hat, labels)\n",
    "        batch_acc = acc(y_hat, labels)\n",
    "\n",
    "        batch_loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "\n",
    "\n",
    "        epoch_loss += batch_loss.item()\n",
    "        epoch_acc += batch_acc.item()\n",
    "\n",
    "    return epoch_loss / len(it), epoch_acc / len(it) # avg loss/acc\n",
    "\n",
    "def eval(model, it, loss, device):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.eval() # set training mode\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for (data, labels) in tqdm(it, desc=\"Evaluating\", leave=False):\n",
    "            data = data.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            y_hat = model(data)\n",
    "            batch_loss = loss(y_hat, labels)\n",
    "            batch_acc = acc(y_hat, labels)\n",
    "\n",
    "            epoch_loss += batch_loss.item()\n",
    "            epoch_acc += batch_acc.item()\n",
    "\n",
    "    return epoch_loss / len(it), epoch_acc / len(it) # avg loss/acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4903ea3553e14bcfb5c6f62386293234",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/69 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5247687704264b8388f6827d780dba45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/704 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec282dad8f6a46d3baa82673086fe6fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 2m 9s\n",
      "\tTrain Loss: 1.619 | Train Acc: 40.68%\n",
      "\t Val. Loss: 1.346 |  Val. Acc: 52.27%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "caa5d45d146441cfa228d9548b3776a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/704 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58b4cc1181b749b59671e1c1fb47c3fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 02 | Epoch Time: 2m 3s\n",
      "\tTrain Loss: 1.257 | Train Acc: 54.87%\n",
      "\t Val. Loss: 1.246 |  Val. Acc: 56.47%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db918b12b1c14d4995ebdcf54e77a2c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/704 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[71], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m trange(EPOCHS, desc\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpochs\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m      4\u001b[0m      start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mmonotonic()\n\u001b[0;32m----> 6\u001b[0m      train_loss, train_acc \u001b[39m=\u001b[39m train(model, train_iterator, optimizer, loss, device)\n\u001b[1;32m      7\u001b[0m      valid_loss, valid_acc \u001b[39m=\u001b[39m \u001b[39meval\u001b[39m(model, valid_iterator, loss, device)\n\u001b[1;32m      8\u001b[0m      end_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mmonotonic()\n",
      "Cell \u001b[0;32mIn[69], line 15\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, it, optim, loss, device)\u001b[0m\n\u001b[1;32m     12\u001b[0m batch_loss \u001b[39m=\u001b[39m loss(y_hat, labels)\n\u001b[1;32m     13\u001b[0m batch_acc \u001b[39m=\u001b[39m acc(y_hat, labels)\n\u001b[0;32m---> 15\u001b[0m batch_loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     16\u001b[0m optim\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     20\u001b[0m epoch_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m batch_loss\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/d2l/lib/python3.9/site-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/d2l/lib/python3.9/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    175\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "EPOCHS = 69\n",
    "\n",
    "for epoch in trange(EPOCHS, desc=\"Epochs\"):\n",
    "     start_time = time.monotonic()\n",
    "\n",
    "     train_loss, train_acc = train(model, train_iterator, optimizer, loss, device)\n",
    "     valid_loss, valid_acc = eval(model, valid_iterator, loss, device)\n",
    "     end_time = time.monotonic()\n",
    "\n",
    "     epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "     print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "     print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "     print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
