{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1f1d4cee-5c4d-44db-8012-2f769cd785ef",
   "metadata": {},
   "source": [
    "# Image Classification CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "b4b1299e-2c81-4fef-8bb5-8b9590ef2b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "from sklearn import decomposition\n",
    "from sklearn import manifold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from tqdm.notebook import tqdm, trange\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import copy\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "1026929f-ed1d-47a9-a804-72f55023287a",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "19dcc198-c158-4f60-a563-583ed66e4848",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Calculated mean: [tensor(0.4914), tensor(0.4822), tensor(0.4465)]\n",
      "Calculated std: [tensor(0.2470), tensor(0.2435), tensor(0.2616)]\n"
     ]
    }
   ],
   "source": [
    "ROOT = '../../data/'\n",
    "\n",
    "train_data = datasets.CIFAR10(root=ROOT,\n",
    "                            train=True,\n",
    "                            download=True)\n",
    "\n",
    "train_data.data = torch.tensor(train_data.data)\n",
    "\n",
    "channels = train_data.data.split(1, dim=-1)\n",
    "channel_tensors = [channel.squeeze(-1) for channel in channels]\n",
    "\n",
    "means = [z.float().mean() / 255 for z in channel_tensors]\n",
    "stds = [z.float().std() / 255 for z in channel_tensors]\n",
    "\n",
    "\n",
    "print(f'Calculated mean: {means}')\n",
    "print(f'Calculated std: {stds}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "00f7657a-2a9f-4ac3-a702-73f83ca72933",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = transforms.Compose([\n",
    "                            # transforms.RandomRotation(5, fill=(0,)),\n",
    "                            # transforms.RandomCrop(28, padding=2),\n",
    "                            transforms.ToTensor(),\n",
    "                            transforms.Normalize(mean=means, std=stds)\n",
    "                                      ])\n",
    "\n",
    "test_transforms = transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize(mean=means, std=stds)\n",
    "                                     ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "f96c2b72-9428-4e07-b5e8-cc922e57f8a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_data = datasets.CIFAR10(root=ROOT,\n",
    "                            train=True,\n",
    "                            download=True,\n",
    "                            transform=train_transforms)\n",
    "\n",
    "test_data = datasets.CIFAR10(root=ROOT,\n",
    "                           train=False,\n",
    "                           download=True,\n",
    "                           transform=test_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "ab6f5a58-ab5d-4797-8dc2-6b6efc071759",
   "metadata": {},
   "outputs": [],
   "source": [
    "VALID_RATIO = 0.9\n",
    "\n",
    "n_train_examples = int(len(train_data) * VALID_RATIO)\n",
    "n_valid_examples = len(train_data) - n_train_examples\n",
    "\n",
    "train_data, valid_data = data.random_split(train_data,\n",
    "                                           [n_train_examples, n_valid_examples])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "f5457d7c-65de-4ee0-88f0-c608fd12923c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 45000\n",
      "Number of validation examples: 5000\n",
      "Number of testing examples: 10000\n"
     ]
    }
   ],
   "source": [
    "valid_data = copy.deepcopy(valid_data)\n",
    "valid_data.dataset.transform = test_transforms\n",
    "\n",
    "print(f'Number of training examples: {len(train_data)}')\n",
    "print(f'Number of validation examples: {len(valid_data)}')\n",
    "print(f'Number of testing examples: {len(test_data)}')\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "train_iterator = data.DataLoader(train_data, batch_size=BATCH_SIZE,shuffle = True)\n",
    "valid_iterator = data.DataLoader(valid_data, batch_size=BATCH_SIZE, shuffle = True)\n",
    "test_iterator = data.DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "2e7ae032-8376-4c3c-b1d9-c23a857867ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 12,496,636 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "class InceptionBlock(nn.Module):\n",
    "    def __init__(self, c1 : float, c2, c3, c4 : float, **kwargs):\n",
    "        super(InceptionBlock, self).__init__(**kwargs)\n",
    "        self.b1 = nn.LazyConv2d(c1, kernel_size=1)\n",
    "        \n",
    "        self.b2a = nn.LazyConv2d(c2[0], kernel_size=1)\n",
    "        self.b2b = nn.LazyConv2d(c2[1], kernel_size=3, padding=1)\n",
    "        \n",
    "        self.b3a = nn.LazyConv2d(c3[0], kernel_size=1)\n",
    "        self.b3b = nn.LazyConv2d(c3[1], kernel_size=5, padding=2)\n",
    "        \n",
    "        self.b4a = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)\n",
    "        self.b4b = nn.LazyConv2d(c4, kernel_size=1)\n",
    "        \n",
    "        \n",
    "    def init(self):\n",
    "        nn.init.xavier_uniform_(self.b1.weight)\n",
    "        nn.init.xavier_uniform_(self.b2a.weight)\n",
    "        nn.init.xavier_uniform_(self.b2b.weight)\n",
    "        nn.init.xavier_uniform_(self.b3a.weight)\n",
    "        nn.init.xavier_uniform_(self.b3b.weight)\n",
    "        nn.init.xavier_uniform_(self.b4b.weight)\n",
    "\n",
    "        \n",
    "    def forward(self, X):\n",
    "        b1 = F.relu(self.b1(X))\n",
    "        b2 = F.relu(self.b2b(F.relu(self.b2a(X))))\n",
    "        b3 = F.relu(self.b3b(F.relu(self.b3a(X))))\n",
    "        b4 = F.relu(self.b4b(F.relu(self.b4a(X))))\n",
    "        return torch.cat((b1,b2,b3,b4),dim=1)\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, output_classes=10):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.LazyConv2d(out_channels=128,kernel_size=11,padding=5)\n",
    "        self.conv2 = nn.LazyConv2d(out_channels = 256, kernel_size=7, padding=3)\n",
    "        self.conv3 = nn.LazyConv2d(out_channels = 512, kernel_size=3, padding = 1)\n",
    "        \n",
    "        self.inc1 = InceptionBlock(64, (64, 128), (128, 256), 128)\n",
    "        self.inc2 = InceptionBlock(96, (64, 216), (128, 354), 216)\n",
    "        self.inc3 = InceptionBlock(128, (64, 256), (128, 512), 256)\n",
    "        \n",
    "        self.MLP = nn.LazyLinear(out_features=1024)\n",
    "        self.end = nn.LazyLinear(out_features=output_classes)\n",
    "        \n",
    "        self.dropout1 = nn.Dropout(p=0.1)\n",
    "        self.dropout2 = nn.Dropout(p=0.5)\n",
    "        \n",
    "\n",
    "\n",
    "    def init_weights(self):\n",
    "        nn.init.xavier_uniform_(self.conv1.weight)\n",
    "        nn.init.xavier_uniform_(self.conv2.weight)\n",
    "        nn.init.xavier_uniform_(self.conv3.weight)\n",
    "        nn.init.xavier_uniform_(self.MLP.weight)\n",
    "        nn.init.xavier_uniform_(self.end.weight)\n",
    "        self.inc1.init()\n",
    "        self.inc2.init()\n",
    "        self.inc3.init()\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.dropout1(nn.ReLU()(self.conv1(X)))\n",
    "        X = nn.MaxPool2d(kernel_size=3,padding=1)(X)\n",
    "        X = self.dropout1(nn.ReLU()(self.conv2(X)))\n",
    "        X = nn.MaxPool2d(kernel_size=3, padding=1)(X)\n",
    "        X = self.dropout1(nn.ReLU()(self.conv3(X)))\n",
    "        X = nn.MaxPool2d(kernel_size=3, padding=1)(X)\n",
    "        \n",
    "        X = self.inc1(X)\n",
    "        X = self.inc2(X)\n",
    "        X = self.inc3(X)\n",
    "    \n",
    "        X = nn.Flatten()(X)\n",
    "        X = nn.ReLU()(self.dropout2(self.MLP(X)))\n",
    "        X = self.dropout2(self.end(X))\n",
    "        return X\n",
    "\n",
    "\n",
    "model = Model()\n",
    "model(torch.Tensor(1,3,32,32))\n",
    "model.init_weights()\n",
    "def init_cnn(module: nn.Module):\n",
    "    if type(module) == nn.Linear or type(module) == nn.Conv2d:\n",
    "        nn.init.xavier_uniform_(module.weight)\n",
    "        \n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "0e0cf11f-4da4-4435-aa93-067f4ce16023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(model.parameters(),lr=0.01)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "device = torch.device('cpu' if torch.backends.mps.is_available() else 'cpu')\n",
    "print(device)\n",
    "model = model.to(device)\n",
    "loss = loss.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "deccf566-501f-46a5-96bd-e9a3ed840677",
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc(y_hat: torch.Tensor, y: torch.Tensor):\n",
    "    preds = y_hat.argmax(1, keepdim=True)\n",
    "    hits = preds.eq(y.view_as(preds)).sum()\n",
    "    return hits.float() / y.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "e466dfad-bcc3-46c6-8883-82f96eaf12b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, it, optim, loss, device):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.train() # set training mode\n",
    "\n",
    "    for (data, labels) in tqdm(it, desc=\"Training\", leave=False):\n",
    "        data = data.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optim.zero_grad()\n",
    "        y_hat = model(data)\n",
    "        batch_loss = loss(y_hat, labels)\n",
    "        batch_acc = acc(y_hat, labels)\n",
    "\n",
    "        batch_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.01)\n",
    "        optim.step()\n",
    "\n",
    "        # print(epoch_loss, batch_loss,y_hat)\n",
    "        \n",
    "        epoch_loss += batch_loss.item()\n",
    "        epoch_acc += batch_acc.item()\n",
    "\n",
    "    return epoch_loss / len(it), epoch_acc / len(it) # avg loss/acc\n",
    "\n",
    "def eval(model, it, loss, device):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.eval() # set training mode\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for (data, labels) in tqdm(it, desc=\"Evaluating\", leave=False):\n",
    "            data = data.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            y_hat = model(data)\n",
    "            batch_loss = loss(y_hat, labels)\n",
    "            batch_acc = acc(y_hat, labels)\n",
    "\n",
    "            epoch_loss += batch_loss.item()\n",
    "            epoch_acc += batch_acc.item()\n",
    "\n",
    "    return epoch_loss / len(it), epoch_acc / len(it) # avg loss/acc\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "b2dda250-1d46-4272-8049-476b4b59868a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11c8e151c1674fb0860de7a044ee7add",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/69 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0907536e5b5f4eb4b9df142837a58308",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/704 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "EPOCHS = 69\n",
    "\n",
    "for epoch in trange(EPOCHS, desc=\"Epochs\"):\n",
    "     start_time = time.monotonic()\n",
    "\n",
    "     train_loss, train_acc = train(model, train_iterator, optimizer, loss, device)\n",
    "     valid_loss, valid_acc = eval(model, valid_iterator, loss, device)\n",
    "     end_time = time.monotonic()\n",
    "\n",
    "     epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "     print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "     print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "     print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c76be4-4bb2-4352-9317-112695ebadef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
